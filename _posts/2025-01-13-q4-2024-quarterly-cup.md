---
layout: page
title: "Forecasting - Metaculus Q4 2024 Retrospective"
time: 7 minutes
published: 2025-01-13
colortags: [{'id': '24a2d473-cb39-4c04-a900-b2eca6ca9437', 'name': 'Metaculus', 'color': 'purple'}, {'id': 'e8f8ec2b-dec7-4d62-98ea-267c6d6bd5cd', 'name': 'Forecasting', 'color': 'default'}, {'id': '11658a68-3c22-4966-b5fe-93f7c296ba7e', 'name': 'Data Analysis', 'color': 'green'}]
tags: ['Metaculus', 'Forecasting', 'Data Analysis']
permalink: q4-2024-quarterly-cup
favicon: q4-2024-quarterly-cup/favicon.png
excerpt: "Looking back at my performance in my first every Metaculus’ Quarterly Cup (Q4 2024), there’s a lot of room left for improvement. This post compares the different forecasting methods I used and what their results were. Thanks to some analysis, I then figure out what I can improve: stick to reasonable probabilities; use fat-tailed distributions to forecast; avoid the trap of nowcasting; update my predictions more often.
This post also explains the fundamentals of log scores and how scoring works on forecasting platforms."
short: False
sourcecode: "https://github.com/obrhubr/metaculus-tournament-export"
hn: 
math: False
image: assets/q4-2024-quarterly-cup/preview.png
---

I recently participated in the [Metaculus Q4 2024 Quarterly Cup](https://www.metaculus.com/tournament/quarterly-cup-2024q4/) which is a forecasting tournament hosted by the forecasting platform [Metaculus](https://www.metaculus.com/). I ranked in the top 15 - out of 379 participants. This sadly means that I didn’t earn one of the prized branded hats, as that would require placing in the top 3…

But I’ve learned some things that should help me improve for the next quarter.

*This post format was inspired by **[xkqr’s Q3 retrospective](https://entropicthoughts.com/quarterly-cup-2024-q3-retrospective)**.*

## What even is Forecasting?

Forecasting itself should be a fairly obvious term, it’s predicting the probability of some event in the future: for example, the probability of Donald Trump winning the election. The platform Metaculus hosts tournaments grouping different questions together, one of which is the quarterly cup. It contains about 50 questions and runs for a period of 90 days.

Scoring a forecast is usually done using a logarithmic score, but the exact implementation varies. As outlined in Metaculus’s [scoring FAQ](https://www.metaculus.com/help/scores-faq/), a **baseline score** is calculated first, which spans from `-897` to `+100` and is logarithmic. By definition, you get **exactly 0 points** if you predict a 50% chance on a binary question (or a uniform distribution on continuous questions). With a better forecast, your score increases.

![<p>Graph showing the baseline relative to the predicted probability for the true outcome of a binary question.</p>](/assets/q4-2024-quarterly-cup/27715f784dba8a9dc4a8c1585374614e.webp)

Because of the characteristics of the logarithmic function, going from 90% to 99.9% certainty will give you very little extra points if you’re right, but in the case you’re wrong, it will cost you *a looot*.

But what actually determines your place in the tournament is your **peer score**. It’s the average difference of your baseline score to the scores of everyone else, which means that it measures your performance relative to others. It can also be negative, which indicates a below average performance.

The aggregate of everyone’s individual forecasts is also calculated for each question, this is called the **community prediction**. Because it takes an average, the peer score of the community forecast is better than that of most individual forecasters (see [Kahneman (2021), Noise](https://en.wikipedia.org/wiki/Noise:_A_Flaw_in_Human_Judgment) for an in depth exploration) - which usually means the community has a positive peer score. Outperforming the community prediction is an indicator of a very good performance.

The image below shows Metaculus’s forecasting UI. The green line is the community prediction, the little orange dots are my predictions. Even though I locked in 0.1% in the end, my baseline score is very negative, as a result of my wrong earlier predictions. This is because being a little wrong weighs a lot more than being very correct (see the log score’s curve above).

![<p>Screenshot of the question “Will OpenAI’s o1 remain the top LLM in all categories of Chatbot Arena on December 30, 2024” on Metaculus.</p>](/assets/q4-2024-quarterly-cup/6866047de0c8583b0652467207ff345b.webp)

## My Results

Metaculus offers an API, with [a public OpenAPI spec](https://www.metaculus.com/api/). I could very easily export my results to analyse them (thanks Metaculus dev team!). If you want to do the same, pull [my scripts from GitHub](https://github.com/obrhubr/metaculus-tournament-export) and get a handy HTML report.

My total peer score is less than that of the community prediction, which scored a total of `692 points` - it would have placed 9th. My goal for next quarter is to perform better than the community overall (I already achieved this for 14 out of 42 questions).

On a positive note, my forecasts were still better than blindly predicting an equal chance for all outcomes, as the sum of my baseline scores is positive. I then labelled all questions based on the method I used to predict and how actively I followed the question as new information came in.

The first graph indicates that mathematical analysis leads to better and more consistent outcomes, which would make sense. It also looks like I made wildly wrong gut feeling guesses on some occasions, which lead to very negative baseline scores. The second graph compares the baseline scores on questions which I actively updated, vs questions I updated once or twice and then mostly forgot. Again, it seems to indicate that active participation leads to a more consistent, positive outcome.

![<p>Graphs comparing my baseline score for questions I analysed vs. used my gut feeling; questions I actively updated vs. questions I only updated once or twice.</p>](/assets/q4-2024-quarterly-cup/a7690c970ca23e5917adc98cd83d254d.webp)

Here, I plotted my peer score for each question instead of my baseline score, which shows relative instead of absolute performance.

![<p>Graphs comparing my peer score for questions I analysed vs. used my gut feeling; questions I actively updated vs. questions I only updated once or twice.</p>](/assets/q4-2024-quarterly-cup/4801bb9e8dd91bc13fb6da12be38c524.webp)

When comparing how much better or worse I was compared to the community peer score, the graphs seem to confirm those conclusions. Once again, questions that I analysed and updated often performed more consistently.

What they also show however, is that the large majority of my points come from questions I used my gut feeling for and then forgot to update (this means I was simply lucky, or worse, incompetent). These were questions where the community was very sure of an outcome which then didn’t come true.

A few examples would be:

1. [What will be the closest color to the 2025 Pantone Color of the Year?](https://www.metaculus.com/questions/29847/) - `+19.3` peer score compared to the community.

1. [Will Astro Bot win the Game of the Year 2024 award?](https://www.metaculus.com/questions/29902/) - `+21.57` points

1. [Will at least one of Andrea Bocelli's concerts at Madison Square Garden on December 18 or 19, 2024 sell out?](https://www.metaculus.com/questions/30252/) - `+48.03` points

## Never make any extreme Predictions

The main improvement I can make is avoiding extreme probabilities - 99.9% or 0.01% - like the plague. Had I held off of these extremes, I could have walked away with a lot of points on at least 10 questions, where the community prediction was confidently wrong. Everyone tells you this, because intellectually everyone knows that it is risky. But in practice, this often gets overlooked: I certainly didn’t listen and paid the price.

For next quarter, I will set myself the hard limit of not going above 92% or below 8%. The upside is very high, the downside very limited. I chose these values according to my very sketchy calculations based on reverse engineering the [scoring math in Metaculus’s source code](https://github.com/Metaculus/metaculus/blob/main/scoring/score_math.py).

If you go with 90% when the community is at 99.9%, you might only be behind by a few points, but if the prediction turns out to be wrong, you stand to gain a few hundred compared to everyone else.

## Gaussians and Nassim Taleb

I used a variation of the bell curve for nearly all my analytical forecasts, mostly in the form of a random walk based on the standard deviation and mean of historical data.

Having recently read [The Black Swan](https://en.wikipedia.org/wiki/The_Black_Swan:_The_Impact_of_the_Highly_Improbable) by [Nassim Taleb](https://en.wikipedia.org/wiki/Nassim_Nicholas_Taleb), this seemed like a typical case of the cognitive dissonance evoked by Taleb in his book. Even though the phenomena in question weren’t at all Gaussian in nature, I still stubbornly stuck to Gaussian methods and tools.

The fat-tailed distributions hailed as the solution by Taleb naturally lead to less extreme probabilities, which is a good thing as we’ve seen above.

## The issue of Nowcasting

To describe the other issue plaguing my predictions, I want to borrow the term [nowcasting](https://en.wikipedia.org/wiki/Nowcasting_(economics)) from meteorology. It’s very easy to fall into the trap of nowcasting when using analytical tools, as illustrated by my attempt at forecasting outlined below.

The question [“Will Bluesky reach 30 million users before 1 January 2025?”](https://www.metaculus.com/questions/30295/will-bluesky-reach-30-million-users-before-1-january-2025/) lent itself ideally to using statistics, as data on new sign-ups was freely available. Thus, I set up a simple polling service, which fed the current sign-up rate into a simple model. It took the current users, the current sign-up rate and extrapolated the probability of reaching 30 million using a random walk.

![<p>Screenshot of the question about Bluesky on the Metaculus site.</p>](/assets/q4-2024-quarterly-cup/acf4d28c7721231d1beadc3ba152ea8f.webp)

As time went on and the sign-ups slowed down as expected, the probability that my model spit out decreased, as is mirrored by the predictions I made on Metaculus.

I think this approach shouldn’t be called forecasting, but rather nowcasting. This is the wrong way to go about it. Instead of naively using the current rate, I should have thought about the different reasons for the sign-up rate to go up or down in the future. Thus, my numbers were correct, but not a real forecast.

I should have **tried to find trends** in the sign-up rate, built different models around those, based on what I thought the sign-up rate would do in the future. I would have arrived at a more realistic prediction much quicker.

## Next Quarter

I urge you to participate in the tournament for Q1 2025 and explore forecasting, as it’s a fun mental exercise and a fun way to do test your data analysis skills. There are questions for everyone, be it geopolitics, economics or more mundane topics. It’s already open now and a few questions are waiting on you.

I’ll be participating as well, doing true **forecasts** (not nowcasts) using fat-tailed distributions and sticking to reasonable probabilities. And I’ll be trying to update my predictions more often…

